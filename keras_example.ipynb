{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USSV_OlCFKOD"
   },
   "source": [
    "# Training a neural network on MNIST with Keras\n",
    "\n",
    "This simple example demonstrates how to plug TensorFlow Datasets (TFDS) into a Keras model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8y9ZkLXmAZc"
   },
   "source": [
    "Copyright 2020 The TensorFlow Datasets Authors, Licensed under the Apache License, Version 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGw9EgE0tC0C"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/datasets/keras_example\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/keras_example.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/datasets/blob/master/docs/keras_example.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/datasets/docs/keras_example.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:29:29.967034Z",
     "iopub.status.busy": "2023-10-03T09:29:29.966435Z",
     "iopub.status.idle": "2023-10-03T09:29:32.828839Z",
     "shell.execute_reply": "2023-10-03T09:29:32.828007Z"
    },
    "id": "TTBSvHcSLBzc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflowNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading tensorflow-2.16.1-cp310-cp310-win_amd64.whl (2.1 kB)\n",
      "Collecting tensorflow-intel==2.16.1\n",
      "  Downloading tensorflow_intel-2.16.1-cp310-cp310-win_amd64.whl (376.9 MB)\n",
      "     -------------------------------------- 376.9/376.9 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\djojo\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.12.2)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Collecting keras>=3.0.0\n",
      "  Using cached keras-3.3.3-py3-none-any.whl (1.1 MB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting numpy<2.0.0,>=1.23.5\n",
      "  Downloading numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "     ---------------------------------------- 15.8/15.8 MB 2.9 MB/s eta 0:00:00\n",
      "Collecting ml-dtypes~=0.3.1\n",
      "  Downloading ml_dtypes-0.3.2-cp310-cp310-win_amd64.whl (127 kB)\n",
      "     -------------------------------------- 127.8/127.8 kB 3.7 MB/s eta 0:00:00\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.64.1-cp310-cp310-win_amd64.whl (4.1 MB)\n",
      "     ---------------------------------------- 4.1/4.1 MB 3.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (58.1.0)\n",
      "Collecting tensorboard<2.17,>=2.16\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-win_amd64.whl (37 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\djojo\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Collecting h5py>=3.10.0\n",
      "  Downloading h5py-3.11.0-cp310-cp310-win_amd64.whl (3.0 MB)\n",
      "     ---------------------------------------- 3.0/3.0 MB 3.9 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Using cached wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "Collecting optree\n",
      "  Downloading optree-0.11.0-cp310-cp310-win_amd64.whl (243 kB)\n",
      "     -------------------------------------- 243.8/243.8 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting namex\n",
      "  Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Collecting rich\n",
      "  Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl (100 kB)\n",
      "     -------------------------------------- 100.3/100.3 kB 1.9 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-win_amd64.whl (17 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\djojo\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.18.0)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, urllib3, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, numpy, mdurl, MarkupSafe, markdown, idna, grpcio, google-pasta, gast, charset-normalizer, certifi, absl-py, werkzeug, requests, opt-einsum, ml-dtypes, markdown-it-py, h5py, astunparse, tensorboard, rich, keras, tensorflow-intel, tensorflow\n",
      "Successfully installed MarkupSafe-2.1.5 absl-py-2.1.0 astunparse-1.6.3 certifi-2024.6.2 charset-normalizer-3.3.2 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 grpcio-1.64.1 h5py-3.11.0 idna-3.7 keras-3.3.3 libclang-18.1.1 markdown-3.6 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.0.8 numpy-1.26.4 opt-einsum-3.3.0 optree-0.11.0 protobuf-4.25.3 requests-2.32.3 rich-13.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-intel-2.16.1 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0 urllib3-2.2.2 werkzeug-3.0.3 wheel-0.43.0 wrapt-1.16.0\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Collecting tensorflow-datasets\n",
      "  Using cached tensorflow_datasets-4.9.6-py3-none-any.whl (5.1 MB)\n",
      "Requirement already satisfied: numpy in c:\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (1.26.4)\n",
      "Collecting simple-parsing\n",
      "  Using cached simple_parsing-0.1.5-py3-none-any.whl (113 kB)\n",
      "Requirement already satisfied: wrapt in c:\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (1.16.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Using cached tensorflow_metadata-1.15.0-py3-none-any.whl (28 kB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.8-cp310-cp310-win_amd64.whl (101 kB)\n",
      "     -------------------------------------- 101.3/101.3 kB 1.9 MB/s eta 0:00:00\n",
      "Collecting etils[enp,epath,epy,etree]>=1.6.0\n",
      "  Downloading etils-1.7.0-py3-none-any.whl (152 kB)\n",
      "     ---------------------------------------- 152.4/152.4 kB ? eta 0:00:00\n",
      "Requirement already satisfied: absl-py in c:\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (2.1.0)\n",
      "Collecting toml\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (4.25.3)\n",
      "Collecting promise\n",
      "  Using cached promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Collecting immutabledict\n",
      "  Using cached immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\djojo\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-datasets) (6.0.0)\n",
      "Requirement already satisfied: termcolor in c:\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (2.4.0)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-16.1.0-cp310-cp310-win_amd64.whl (25.9 MB)\n",
      "     ---------------------------------------- 25.9/25.9 MB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (2.32.3)\n",
      "Collecting zipp\n",
      "  Using cached zipp-3.19.2-py3-none-any.whl (9.0 kB)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2024.6.0-py3-none-any.whl (176 kB)\n",
      "Collecting importlib_resources\n",
      "  Using cached importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\djojo\\appdata\\roaming\\python\\python310\\site-packages (from etils[enp,epath,epy,etree]>=1.6.0->tensorflow-datasets) (4.12.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\python310\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\python310\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\python310\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2024.6.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\python310\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.2)\n",
      "Requirement already satisfied: colorama in c:\\python\\python310\\lib\\site-packages (from click->tensorflow-datasets) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\djojo\\appdata\\roaming\\python\\python310\\site-packages (from promise->tensorflow-datasets) (1.16.0)\n",
      "Collecting docstring-parser~=0.15\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Collecting protobuf>=3.20\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-win_amd64.whl (904 kB)\n",
      "     -------------------------------------- 904.0/904.0 kB 4.8 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21563 sha256=3e85b2e5aba3cdc2f57f43e3f7e0e5366b392cd9dfdc55b588e953956a9e768d\n",
      "  Stored in directory: c:\\users\\djojo\\appdata\\local\\pip\\cache\\wheels\\54\\4e\\28\\3ed0e1c8a752867445bab994d2340724928aa3ab059c57c8db\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, zipp, tqdm, toml, pyarrow, protobuf, promise, importlib_resources, immutabledict, fsspec, etils, docstring-parser, click, tensorflow-metadata, simple-parsing, tensorflow-datasets\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "Successfully installed click-8.1.7 dm-tree-0.1.8 docstring-parser-0.16 etils-1.7.0 fsspec-2024.6.0 immutabledict-4.2.0 importlib_resources-6.4.0 promise-2.3 protobuf-3.20.3 pyarrow-16.1.0 simple-parsing-0.1.5 tensorflow-datasets-4.9.6 tensorflow-metadata-1.15.0 toml-0.10.2 tqdm-4.66.4 zipp-3.19.2\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow\n",
    "%pip install tensorflow-datasets\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjI6VgOBf0v0"
   },
   "source": [
    "## Step 1: Create your input pipeline\n",
    "\n",
    "Start by building an efficient input pipeline using advices from:\n",
    "* The [Performance tips](https://www.tensorflow.org/datasets/performances) guide\n",
    "* The [Better performance with the `tf.data` API](https://www.tensorflow.org/guide/data_performance#optimize_performance) guide\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3aH3vP_XLI8"
   },
   "source": [
    "### Load a dataset\n",
    "\n",
    "Load the MNIST dataset with the following arguments:\n",
    "\n",
    "* `shuffle_files=True`: The MNIST data is only stored in a single file, but for larger datasets with multiple files on disk, it's good practice to shuffle them when training.\n",
    "* `as_supervised=True`: Returns a tuple `(img, label)` instead of a dictionary `{'image': img, 'label': label}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:29:32.833624Z",
     "iopub.status.busy": "2023-10-03T09:29:32.832834Z",
     "iopub.status.idle": "2023-10-03T09:29:33.807527Z",
     "shell.execute_reply": "2023-10-03T09:29:33.806662Z"
    },
    "id": "ZUMhCXhFXdHQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\djojo\\tensorflow_datasets\\mnist\\3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/2 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/3 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:00,  4.03 url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:00,  3.44 url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  2.92 url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  2.78 url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  1.72 url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  1.67 url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  1.61 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  2.80 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  3.59 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  3.30 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  3.24 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  3.15 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  3.10 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  2.75 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  2.18 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  1.75 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:02<00:00,  1.45 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:02<00:00,  1.24 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:02<00:00,  1.08 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:03<00:01,  1.04s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:03<00:01,  1.16s/ url]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:03<00:01,  1.28s/ url]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:04<00:00,  1.00 url/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:04<00:00,  1.00 url/s]\n",
      "\u001b[A\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:04<00:00,  1.00 url/s]\n",
      "Extraction completed...: 100%|██████████| 4/4 [00:04<00:00,  1.12s/ file]\n",
      "Dl Size...: 100%|██████████| 10/10 [00:04<00:00,  2.23 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:04<00:00,  1.12s/ url]\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to C:\\Users\\djojo\\tensorflow_datasets\\mnist\\3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgwCFAcWXQTx"
   },
   "source": [
    "### Build a training pipeline\n",
    "\n",
    "Apply the following transformations:\n",
    "\n",
    "* `tf.data.Dataset.map`: TFDS provide images of type `tf.uint8`, while the model expects `tf.float32`. Therefore, you need to normalize images.\n",
    "* `tf.data.Dataset.cache` As you fit the dataset in memory, cache it before shuffling for a better performance.<br/>\n",
    "__Note:__ Random transformations should be applied after caching.\n",
    "* `tf.data.Dataset.shuffle`: For true randomness, set the shuffle buffer to the full dataset size.<br/>\n",
    "__Note:__ For large datasets that can't fit in memory, use `buffer_size=1000` if your system allows it.\n",
    "* `tf.data.Dataset.batch`: Batch elements of the dataset after shuffling to get unique batches at each epoch.\n",
    "* `tf.data.Dataset.prefetch`: It is good practice to end the pipeline by prefetching [for performance](https://www.tensorflow.org/guide/data_performance#prefetching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:29:33.811770Z",
     "iopub.status.busy": "2023-10-03T09:29:33.811063Z",
     "iopub.status.idle": "2023-10-03T09:29:33.865697Z",
     "shell.execute_reply": "2023-10-03T09:29:33.865011Z"
    },
    "id": "haykx2K9XgiI"
   },
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(128)\n",
    "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbsMy4X1XVFv"
   },
   "source": [
    "### Build an evaluation pipeline\n",
    "\n",
    "Your testing pipeline is similar to the training pipeline with small differences:\n",
    "\n",
    " * You don't need to call `tf.data.Dataset.shuffle`.\n",
    " * Caching is done after batching because batches can be the same between epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:29:33.869308Z",
     "iopub.status.busy": "2023-10-03T09:29:33.868703Z",
     "iopub.status.idle": "2023-10-03T09:29:33.883087Z",
     "shell.execute_reply": "2023-10-03T09:29:33.882431Z"
    },
    "id": "A0KjuDf7XiqY"
   },
   "outputs": [],
   "source": [
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.batch(128)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTFoji3INMEM"
   },
   "source": [
    "## Step 2: Create and train the model\n",
    "\n",
    "Plug the TFDS input pipeline into a simple Keras model, compile the model, and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-03T09:29:33.886520Z",
     "iopub.status.busy": "2023-10-03T09:29:33.885961Z",
     "iopub.status.idle": "2023-10-03T09:29:44.126728Z",
     "shell.execute_reply": "2023-10-03T09:29:44.125806Z"
    },
    "id": "XWqxdmS1NLKA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 0.6309 - sparse_categorical_accuracy: 0.8232 - val_loss: 0.1924 - val_sparse_categorical_accuracy: 0.9442\n",
      "Epoch 2/6\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.1792 - sparse_categorical_accuracy: 0.9487 - val_loss: 0.1341 - val_sparse_categorical_accuracy: 0.9594\n",
      "Epoch 3/6\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.1200 - sparse_categorical_accuracy: 0.9657 - val_loss: 0.1082 - val_sparse_categorical_accuracy: 0.9673\n",
      "Epoch 4/6\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0933 - sparse_categorical_accuracy: 0.9737 - val_loss: 0.0938 - val_sparse_categorical_accuracy: 0.9715\n",
      "Epoch 5/6\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0761 - sparse_categorical_accuracy: 0.9788 - val_loss: 0.0883 - val_sparse_categorical_accuracy: 0.9730\n",
      "Epoch 6/6\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.0568 - sparse_categorical_accuracy: 0.9840 - val_loss: 0.0789 - val_sparse_categorical_accuracy: 0.9746\n",
      "Keras Accuracy:  97.46000170707703\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    epochs=6,\n",
    "    validation_data=ds_test,\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "scores = model.evaluate(ds_test, verbose=0)\n",
    "print(\"Keras Accuracy: \", scores[1]*100)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tensorflow/datasets",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
